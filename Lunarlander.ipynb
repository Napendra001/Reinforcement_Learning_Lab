{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rocket trajectory optimization is a classic topic in Optimal Control.\n",
    "\n",
    "According to Pontryagin's maximum principle it's optimal to fire engine full throttle or\n",
    "turn it off. That's the reason this environment is OK to have discreet actions (engine on or off).\n",
    "\n",
    "The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector.\n",
    "Reward for moving from the top of the screen to the landing pad and zero speed is about 100..140 points.\n",
    "If the lander moves away from the landing pad it loses reward. The episode finishes if the lander crashes or\n",
    "comes to rest, receiving an additional -100 or +100 points. Each leg with ground contact is +10 points.\n",
    "Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame.\n",
    "Solved is 200 points.\n",
    "\n",
    "Landing outside the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land\n",
    "on its first attempt. Please see the source code for details.\n",
    "\n",
    "Created by Oleg Klimov. Licensed on the same terms as the rest of OpenAI Gym.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import Box2D\n",
    "from Box2D.b2 import (\n",
    "    edgeShape,\n",
    "    circleShape,\n",
    "    fixtureDef,\n",
    "    polygonShape,\n",
    "    revoluteJointDef,\n",
    "    contactListener,\n",
    ")\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding, EzPickle\n",
    "\n",
    "FPS = 50\n",
    "SCALE = 30.0  # affects how fast-paced the game is, forces should be adjusted as well\n",
    "\n",
    "MAIN_ENGINE_POWER = 13.0\n",
    "SIDE_ENGINE_POWER = 0.6\n",
    "\n",
    "INITIAL_RANDOM = 1000.0  # Set 1500 to make game harder\n",
    "\n",
    "LANDER_POLY = [(-14, +17), (-17, 0), (-17, -10), (+17, -10), (+17, 0), (+14, +17)]\n",
    "LEG_AWAY = 20\n",
    "LEG_DOWN = 18\n",
    "LEG_W, LEG_H = 2, 8\n",
    "LEG_SPRING_TORQUE = 40\n",
    "\n",
    "SIDE_ENGINE_HEIGHT = 14.0\n",
    "SIDE_ENGINE_AWAY = 12.0\n",
    "\n",
    "VIEWPORT_W = 600\n",
    "VIEWPORT_H = 400\n",
    "\n",
    "\n",
    "class ContactDetector(contactListener):\n",
    "    def __init__(self, env):\n",
    "        contactListener.__init__(self)\n",
    "        self.env = env\n",
    "\n",
    "    def BeginContact(self, contact):\n",
    "        if (\n",
    "            self.env.lander == contact.fixtureA.body\n",
    "            or self.env.lander == contact.fixtureB.body\n",
    "        ):\n",
    "            self.env.game_over = True\n",
    "        for i in range(2):\n",
    "            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n",
    "                self.env.legs[i].ground_contact = True\n",
    "\n",
    "    def EndContact(self, contact):\n",
    "        for i in range(2):\n",
    "            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n",
    "                self.env.legs[i].ground_contact = False\n",
    "\n",
    "\n",
    "class LunarLander(gym.Env, EzPickle):\n",
    "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": FPS}\n",
    "\n",
    "    continuous = False\n",
    "\n",
    "    def __init__(self):\n",
    "        EzPickle.__init__(self)\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "\n",
    "        self.world = Box2D.b2World()\n",
    "        self.moon = None\n",
    "        self.lander = None\n",
    "        self.particles = []\n",
    "\n",
    "        self.prev_reward = None\n",
    "\n",
    "        # useful range is -1 .. +1, but spikes can be higher\n",
    "        self.observation_space = spaces.Box(\n",
    "            -np.inf, np.inf, shape=(8,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        if self.continuous:\n",
    "            # Action is two floats [main engine, left-right engines].\n",
    "            # Main engine: -1..0 off, 0..+1 throttle from 50% to 100% power. Engine can't work with less than 50% power.\n",
    "            # Left-right:  -1.0..-0.5 fire left engine, +0.5..+1.0 fire right engine, -0.5..0.5 off\n",
    "            self.action_space = spaces.Box(-1, +1, (2,), dtype=np.float32)\n",
    "        else:\n",
    "            # Nop, fire left engine, main engine, right engine\n",
    "            self.action_space = spaces.Discrete(4)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def _destroy(self):\n",
    "        if not self.moon:\n",
    "            return\n",
    "        self.world.contactListener = None\n",
    "        self._clean_particles(True)\n",
    "        self.world.DestroyBody(self.moon)\n",
    "        self.moon = None\n",
    "        self.world.DestroyBody(self.lander)\n",
    "        self.lander = None\n",
    "        self.world.DestroyBody(self.legs[0])\n",
    "        self.world.DestroyBody(self.legs[1])\n",
    "\n",
    "    def reset(self):\n",
    "        self._destroy()\n",
    "        self.world.contactListener_keepref = ContactDetector(self)\n",
    "        self.world.contactListener = self.world.contactListener_keepref\n",
    "        self.game_over = False\n",
    "        self.prev_shaping = None\n",
    "\n",
    "        W = VIEWPORT_W / SCALE\n",
    "        H = VIEWPORT_H / SCALE\n",
    "\n",
    "        # terrain\n",
    "        CHUNKS = 11\n",
    "        height = self.np_random.uniform(0, H / 2, size=(CHUNKS + 1,))\n",
    "        chunk_x = [W / (CHUNKS - 1) * i for i in range(CHUNKS)]\n",
    "        self.helipad_x1 = chunk_x[CHUNKS // 2 - 1]\n",
    "        self.helipad_x2 = chunk_x[CHUNKS // 2 + 1]\n",
    "        self.helipad_y = H / 4\n",
    "        height[CHUNKS // 2 - 2] = self.helipad_y\n",
    "        height[CHUNKS // 2 - 1] = self.helipad_y\n",
    "        height[CHUNKS // 2 + 0] = self.helipad_y\n",
    "        height[CHUNKS // 2 + 1] = self.helipad_y\n",
    "        height[CHUNKS // 2 + 2] = self.helipad_y\n",
    "        smooth_y = [\n",
    "            0.33 * (height[i - 1] + height[i + 0] + height[i + 1])\n",
    "            for i in range(CHUNKS)\n",
    "        ]\n",
    "\n",
    "        self.moon = self.world.CreateStaticBody(\n",
    "            shapes=edgeShape(vertices=[(0, 0), (W, 0)])\n",
    "        )\n",
    "        self.sky_polys = []\n",
    "        for i in range(CHUNKS - 1):\n",
    "            p1 = (chunk_x[i], smooth_y[i])\n",
    "            p2 = (chunk_x[i + 1], smooth_y[i + 1])\n",
    "            self.moon.CreateEdgeFixture(vertices=[p1, p2], density=0, friction=0.1)\n",
    "            self.sky_polys.append([p1, p2, (p2[0], H), (p1[0], H)])\n",
    "\n",
    "        self.moon.color1 = (0.0, 0.0, 0.0)\n",
    "        self.moon.color2 = (0.0, 0.0, 0.0)\n",
    "\n",
    "        initial_y = VIEWPORT_H / SCALE\n",
    "        self.lander = self.world.CreateDynamicBody(\n",
    "            position=(VIEWPORT_W / SCALE / 2, initial_y),\n",
    "            angle=0.0,\n",
    "            fixtures=fixtureDef(\n",
    "                shape=polygonShape(\n",
    "                    vertices=[(x / SCALE, y / SCALE) for x, y in LANDER_POLY]\n",
    "                ),\n",
    "                density=5.0,\n",
    "                friction=0.1,\n",
    "                categoryBits=0x0010,\n",
    "                maskBits=0x001,  # collide only with ground\n",
    "                restitution=0.0,\n",
    "            ),  # 0.99 bouncy\n",
    "        )\n",
    "        self.lander.color1 = (0.5, 0.4, 0.9)\n",
    "        self.lander.color2 = (0.3, 0.3, 0.5)\n",
    "        self.lander.ApplyForceToCenter(\n",
    "            (\n",
    "                self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n",
    "                self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n",
    "            ),\n",
    "            True,\n",
    "        )\n",
    "\n",
    "        self.legs = []\n",
    "        for i in [-1, +1]:\n",
    "            leg = self.world.CreateDynamicBody(\n",
    "                position=(VIEWPORT_W / SCALE / 2 - i * LEG_AWAY / SCALE, initial_y),\n",
    "                angle=(i * 0.05),\n",
    "                fixtures=fixtureDef(\n",
    "                    shape=polygonShape(box=(LEG_W / SCALE, LEG_H / SCALE)),\n",
    "                    density=1.0,\n",
    "                    restitution=0.0,\n",
    "                    categoryBits=0x0020,\n",
    "                    maskBits=0x001,\n",
    "                ),\n",
    "            )\n",
    "            leg.ground_contact = False\n",
    "            leg.color1 = (0.5, 0.4, 0.9)\n",
    "            leg.color2 = (0.3, 0.3, 0.5)\n",
    "            rjd = revoluteJointDef(\n",
    "                bodyA=self.lander,\n",
    "                bodyB=leg,\n",
    "                localAnchorA=(0, 0),\n",
    "                localAnchorB=(i * LEG_AWAY / SCALE, LEG_DOWN / SCALE),\n",
    "                enableMotor=True,\n",
    "                enableLimit=True,\n",
    "                maxMotorTorque=LEG_SPRING_TORQUE,\n",
    "                motorSpeed=+0.3 * i,  # low enough not to jump back into the sky\n",
    "            )\n",
    "            if i == -1:\n",
    "                rjd.lowerAngle = (\n",
    "                    +0.9 - 0.5\n",
    "                )  # The most esoteric numbers here, angled legs have freedom to travel within\n",
    "                rjd.upperAngle = +0.9\n",
    "            else:\n",
    "                rjd.lowerAngle = -0.9\n",
    "                rjd.upperAngle = -0.9 + 0.5\n",
    "            leg.joint = self.world.CreateJoint(rjd)\n",
    "            self.legs.append(leg)\n",
    "\n",
    "        self.drawlist = [self.lander] + self.legs\n",
    "\n",
    "        return self.step(np.array([0, 0]) if self.continuous else 0)[0]\n",
    "\n",
    "    def _create_particle(self, mass, x, y, ttl):\n",
    "        p = self.world.CreateDynamicBody(\n",
    "            position=(x, y),\n",
    "            angle=0.0,\n",
    "            fixtures=fixtureDef(\n",
    "                shape=circleShape(radius=2 / SCALE, pos=(0, 0)),\n",
    "                density=mass,\n",
    "                friction=0.1,\n",
    "                categoryBits=0x0100,\n",
    "                maskBits=0x001,  # collide only with ground\n",
    "                restitution=0.3,\n",
    "            ),\n",
    "        )\n",
    "        p.ttl = ttl\n",
    "        self.particles.append(p)\n",
    "        self._clean_particles(False)\n",
    "        return p\n",
    "\n",
    "    def _clean_particles(self, all):\n",
    "        while self.particles and (all or self.particles[0].ttl < 0):\n",
    "            self.world.DestroyBody(self.particles.pop(0))\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.continuous:\n",
    "            action = np.clip(action, -1, +1).astype(np.float32)\n",
    "        else:\n",
    "            assert self.action_space.contains(action), \"%r (%s) invalid \" % (\n",
    "                action,\n",
    "                type(action),\n",
    "            )\n",
    "\n",
    "        # Engines\n",
    "        tip = (math.sin(self.lander.angle), math.cos(self.lander.angle))\n",
    "        side = (-tip[1], tip[0])\n",
    "        dispersion = [self.np_random.uniform(-1.0, +1.0) / SCALE for _ in range(2)]\n",
    "\n",
    "        m_power = 0.0\n",
    "        if (self.continuous and action[0] > 0.0) or (\n",
    "            not self.continuous and action == 2\n",
    "        ):\n",
    "            # Main engine\n",
    "            if self.continuous:\n",
    "                m_power = (np.clip(action[0], 0.0, 1.0) + 1.0) * 0.5  # 0.5..1.0\n",
    "                assert m_power >= 0.5 and m_power <= 1.0\n",
    "            else:\n",
    "                m_power = 1.0\n",
    "            ox = (\n",
    "                tip[0] * (4 / SCALE + 2 * dispersion[0]) + side[0] * dispersion[1]\n",
    "            )  # 4 is move a bit downwards, +-2 for randomness\n",
    "            oy = -tip[1] * (4 / SCALE + 2 * dispersion[0]) - side[1] * dispersion[1]\n",
    "            impulse_pos = (self.lander.position[0] + ox, self.lander.position[1] + oy)\n",
    "            p = self._create_particle(\n",
    "                3.5,  # 3.5 is here to make particle speed adequate\n",
    "                impulse_pos[0],\n",
    "                impulse_pos[1],\n",
    "                m_power,\n",
    "            )  # particles are just a decoration\n",
    "            p.ApplyLinearImpulse(\n",
    "                (ox * MAIN_ENGINE_POWER * m_power, oy * MAIN_ENGINE_POWER * m_power),\n",
    "                impulse_pos,\n",
    "                True,\n",
    "            )\n",
    "            self.lander.ApplyLinearImpulse(\n",
    "                (-ox * MAIN_ENGINE_POWER * m_power, -oy * MAIN_ENGINE_POWER * m_power),\n",
    "                impulse_pos,\n",
    "                True,\n",
    "            )\n",
    "\n",
    "        s_power = 0.0\n",
    "        if (self.continuous and np.abs(action[1]) > 0.5) or (\n",
    "            not self.continuous and action in [1, 3]\n",
    "        ):\n",
    "            # Orientation engines\n",
    "            if self.continuous:\n",
    "                direction = np.sign(action[1])\n",
    "                s_power = np.clip(np.abs(action[1]), 0.5, 1.0)\n",
    "                assert s_power >= 0.5 and s_power <= 1.0\n",
    "            else:\n",
    "                direction = action - 2\n",
    "                s_power = 1.0\n",
    "            ox = tip[0] * dispersion[0] + side[0] * (\n",
    "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
    "            )\n",
    "            oy = -tip[1] * dispersion[0] - side[1] * (\n",
    "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
    "            )\n",
    "            impulse_pos = (\n",
    "                self.lander.position[0] + ox - tip[0] * 17 / SCALE,\n",
    "                self.lander.position[1] + oy + tip[1] * SIDE_ENGINE_HEIGHT / SCALE,\n",
    "            )\n",
    "            p = self._create_particle(0.7, impulse_pos[0], impulse_pos[1], s_power)\n",
    "            p.ApplyLinearImpulse(\n",
    "                (ox * SIDE_ENGINE_POWER * s_power, oy * SIDE_ENGINE_POWER * s_power),\n",
    "                impulse_pos,\n",
    "                True,\n",
    "            )\n",
    "            self.lander.ApplyLinearImpulse(\n",
    "                (-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),\n",
    "                impulse_pos,\n",
    "                True,\n",
    "            )\n",
    "\n",
    "        self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)\n",
    "\n",
    "        pos = self.lander.position\n",
    "        vel = self.lander.linearVelocity\n",
    "        state = [\n",
    "            (pos.x - VIEWPORT_W / SCALE / 2) / (VIEWPORT_W / SCALE / 2),\n",
    "            (pos.y - (self.helipad_y + LEG_DOWN / SCALE)) / (VIEWPORT_H / SCALE / 2),\n",
    "            vel.x * (VIEWPORT_W / SCALE / 2) / FPS,\n",
    "            vel.y * (VIEWPORT_H / SCALE / 2) / FPS,\n",
    "            self.lander.angle,\n",
    "            20.0 * self.lander.angularVelocity / FPS,\n",
    "            1.0 if self.legs[0].ground_contact else 0.0,\n",
    "            1.0 if self.legs[1].ground_contact else 0.0,\n",
    "        ]\n",
    "        assert len(state) == 8\n",
    "\n",
    "        reward = 0\n",
    "        shaping = (\n",
    "            -100 * np.sqrt(state[0] * state[0] + state[1] * state[1])\n",
    "            - 100 * np.sqrt(state[2] * state[2] + state[3] * state[3])\n",
    "            - 100 * abs(state[4])\n",
    "            + 10 * state[6]\n",
    "            + 10 * state[7]\n",
    "        )  # And ten points for legs contact, the idea is if you\n",
    "        # lose contact again after landing, you get negative reward\n",
    "        if self.prev_shaping is not None:\n",
    "            reward = shaping - self.prev_shaping\n",
    "        self.prev_shaping = shaping\n",
    "\n",
    "        reward -= (\n",
    "            m_power * 0.30\n",
    "        )  # less fuel spent is better, about -30 for heuristic landing\n",
    "        reward -= s_power * 0.03\n",
    "\n",
    "        done = False\n",
    "        if self.game_over or abs(state[0]) >= 1.0:\n",
    "            done = True\n",
    "            reward = -100\n",
    "        if not self.lander.awake:\n",
    "            done = True\n",
    "            reward = +100\n",
    "        return np.array(state, dtype=np.float32), reward, done, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        from gym.envs.classic_control import rendering\n",
    "\n",
    "        if self.viewer is None:\n",
    "            self.viewer = rendering.Viewer(VIEWPORT_W, VIEWPORT_H)\n",
    "            self.viewer.set_bounds(0, VIEWPORT_W / SCALE, 0, VIEWPORT_H / SCALE)\n",
    "\n",
    "        for obj in self.particles:\n",
    "            obj.ttl -= 0.15\n",
    "            obj.color1 = (\n",
    "                max(0.2, 0.2 + obj.ttl),\n",
    "                max(0.2, 0.5 * obj.ttl),\n",
    "                max(0.2, 0.5 * obj.ttl),\n",
    "            )\n",
    "            obj.color2 = (\n",
    "                max(0.2, 0.2 + obj.ttl),\n",
    "                max(0.2, 0.5 * obj.ttl),\n",
    "                max(0.2, 0.5 * obj.ttl),\n",
    "            )\n",
    "\n",
    "        self._clean_particles(False)\n",
    "\n",
    "        for p in self.sky_polys:\n",
    "            self.viewer.draw_polygon(p, color=(0, 0, 0))\n",
    "\n",
    "        for obj in self.particles + self.drawlist:\n",
    "            for f in obj.fixtures:\n",
    "                trans = f.body.transform\n",
    "                if type(f.shape) is circleShape:\n",
    "                    t = rendering.Transform(translation=trans * f.shape.pos)\n",
    "                    self.viewer.draw_circle(\n",
    "                        f.shape.radius, 20, color=obj.color1\n",
    "                    ).add_attr(t)\n",
    "                    self.viewer.draw_circle(\n",
    "                        f.shape.radius, 20, color=obj.color2, filled=False, linewidth=2\n",
    "                    ).add_attr(t)\n",
    "                else:\n",
    "                    path = [trans * v for v in f.shape.vertices]\n",
    "                    self.viewer.draw_polygon(path, color=obj.color1)\n",
    "                    path.append(path[0])\n",
    "                    self.viewer.draw_polyline(path, color=obj.color2, linewidth=2)\n",
    "\n",
    "        for x in [self.helipad_x1, self.helipad_x2]:\n",
    "            flagy1 = self.helipad_y\n",
    "            flagy2 = flagy1 + 50 / SCALE\n",
    "            self.viewer.draw_polyline([(x, flagy1), (x, flagy2)], color=(1, 1, 1))\n",
    "            self.viewer.draw_polygon(\n",
    "                [\n",
    "                    (x, flagy2),\n",
    "                    (x, flagy2 - 10 / SCALE),\n",
    "                    (x + 25 / SCALE, flagy2 - 5 / SCALE),\n",
    "                ],\n",
    "                color=(0.8, 0.8, 0),\n",
    "            )\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n",
    "\n",
    "\n",
    "class LunarLanderContinuous(LunarLander):\n",
    "    continuous = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "St\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-17 12:25:55.974601: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-11-17 12:25:55.974683: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (NAPENDRA): /proc/driver/nvidia/version does not exist\n",
      "2021-11-17 12:25:55.976816: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/napendra/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               4608      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 136,964\n",
      "Trainable params: 136,964\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/napendra/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/napendra/.local/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "2021-11-17 12:25:57.680113: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t: Episode || Reward:  -413.3464569874839 \t|| Average Reward:  -413.3464569874839 \t epsilon:  0.995\n",
      "1 \t: Episode || Reward:  -435.6143541418218 \t|| Average Reward:  -424.48040556465287 \t epsilon:  0.990025\n",
      "2 \t: Episode || Reward:  -180.0732422513895 \t|| Average Reward:  -343.01135112689843 \t epsilon:  0.985074875\n",
      "3 \t: Episode || Reward:  -297.9978876766248 \t|| Average Reward:  -331.75798526433005 \t epsilon:  0.9801495006250001\n",
      "4 \t: Episode || Reward:  -208.97687938639677 \t|| Average Reward:  -307.2017640887434 \t epsilon:  0.9752487531218751\n",
      "5 \t: Episode || Reward:  -401.7204651633061 \t|| Average Reward:  -322.9548809345038 \t epsilon:  0.9703725093562657\n",
      "6 \t: Episode || Reward:  -119.42992405150196 \t|| Average Reward:  -293.879887094075 \t epsilon:  0.9655206468094844\n",
      "7 \t: Episode || Reward:  -342.2070235194025 \t|| Average Reward:  -299.9207791472409 \t epsilon:  0.960693043575437\n",
      "8 \t: Episode || Reward:  -132.71793441271683 \t|| Average Reward:  -281.34268528784935 \t epsilon:  0.9558895783575597\n",
      "9 \t: Episode || Reward:  -130.83679965974127 \t|| Average Reward:  -266.2920967250385 \t epsilon:  0.9511101304657719\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.activations import relu, linear\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, env, lr, gamma, epsilon, epsilon_decay):\n",
    "\n",
    "        self.env = env\n",
    "        self.action_space = env.action_space\n",
    "        self.observation_space = env.observation_space\n",
    "        self.counter = 0\n",
    "\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.rewards_list = []\n",
    "\n",
    "        self.replay_memory_buffer = deque(maxlen=500000)\n",
    "        self.batch_size = 64\n",
    "        self.epsilon_min = 0.01\n",
    "        self.num_action_space = self.action_space.n\n",
    "        self.num_observation_space = env.observation_space.shape[0]\n",
    "        self.model = self.initialize_model()\n",
    "\n",
    "    def initialize_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(512, input_dim=self.num_observation_space, activation=relu))\n",
    "        model.add(Dense(256, activation=relu))\n",
    "        model.add(Dense(self.num_action_space, activation=linear))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss=mean_squared_error,optimizer=Adam(lr=self.lr))\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.num_action_space)\n",
    "\n",
    "        predicted_actions = self.model.predict(state)\n",
    "        return np.argmax(predicted_actions[0])\n",
    "\n",
    "    def add_to_replay_memory(self, state, action, reward, next_state, done):\n",
    "        self.replay_memory_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def learn_and_update_weights_by_reply(self):\n",
    "\n",
    "        # replay_memory_buffer size check\n",
    "        if len(self.replay_memory_buffer) < self.batch_size or self.counter != 0:\n",
    "            return\n",
    "\n",
    "        # Early Stopping\n",
    "        if np.mean(self.rewards_list[-10:]) > 180:\n",
    "            return\n",
    "\n",
    "        random_sample = self.get_random_sample_from_replay_mem()\n",
    "        states, actions, rewards, next_states, done_list = self.get_attribues_from_sample(random_sample)\n",
    "        targets = rewards + self.gamma * (np.amax(self.model.predict_on_batch(next_states), axis=1)) * (1 - done_list)\n",
    "        target_vec = self.model.predict_on_batch(states)\n",
    "        indexes = np.array([i for i in range(self.batch_size)])\n",
    "        target_vec[[indexes], [actions]] = targets\n",
    "\n",
    "        self.model.fit(states, target_vec, epochs=1, verbose=0)\n",
    "\n",
    "    def get_attribues_from_sample(self, random_sample):\n",
    "        states = np.array([i[0] for i in random_sample])\n",
    "        actions = np.array([i[1] for i in random_sample])\n",
    "        rewards = np.array([i[2] for i in random_sample])\n",
    "        next_states = np.array([i[3] for i in random_sample])\n",
    "        done_list = np.array([i[4] for i in random_sample])\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "        return np.squeeze(states), actions, rewards, next_states, done_list\n",
    "\n",
    "    def get_random_sample_from_replay_mem(self):\n",
    "        random_sample = random.sample(self.replay_memory_buffer, self.batch_size)\n",
    "        return random_sample\n",
    "\n",
    "    def train(self, num_episodes=2000, can_stop=True):\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            reward_for_episode = 0\n",
    "            num_steps = 1000\n",
    "            state = np.reshape(state, [1, self.num_observation_space])\n",
    "            for step in range(num_steps):\n",
    "                env.render()\n",
    "                received_action = self.get_action(state)\n",
    "                # print(\"received_action:\", received_action)\n",
    "                next_state, reward, done, info = env.step(received_action)\n",
    "                next_state = np.reshape(next_state, [1, self.num_observation_space])\n",
    "                # Store the experience in replay memory\n",
    "                self.add_to_replay_memory(state, received_action, reward, next_state, done)\n",
    "                # add up rewards\n",
    "                reward_for_episode += reward\n",
    "                state = next_state\n",
    "                self.update_counter()\n",
    "                self.learn_and_update_weights_by_reply()\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "            self.rewards_list.append(reward_for_episode)\n",
    "\n",
    "            # Decay the epsilon after each experience completion\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "\n",
    "            # Check for breaking condition\n",
    "            last_rewards_mean = np.mean(self.rewards_list[-100:])\n",
    "            if last_rewards_mean > 200 and can_stop:\n",
    "                print(\"DQN Training Complete...\")\n",
    "                break\n",
    "            print(episode, \"\\t: Episode || Reward: \",reward_for_episode, \"\\t|| Average Reward: \",last_rewards_mean, \"\\t epsilon: \", self.epsilon )\n",
    "\n",
    "    def update_counter(self):\n",
    "        self.counter += 1\n",
    "        step_size = 5\n",
    "        self.counter = self.counter % step_size\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "\n",
    "\n",
    "def test_already_trained_model(trained_model):\n",
    "    rewards_list = []\n",
    "    num_test_episode = 100\n",
    "    #env = gym.make(\"LunarLander-v2\")\n",
    "    \n",
    "    print(\"Starting Testing of the trained model...\")\n",
    "\n",
    "    step_count = 1000\n",
    "\n",
    "    for test_episode in range(num_test_episode):\n",
    "        current_state = env.reset()\n",
    "        num_observation_space = env.observation_space.shape[0]\n",
    "        current_state = np.reshape(current_state, [1, num_observation_space])\n",
    "        reward_for_episode = 0\n",
    "        for step in range(step_count):\n",
    "            env.render()\n",
    "            selected_action = np.argmax(trained_model.predict(current_state)[0])\n",
    "            new_state, reward, done, info = env.step(selected_action)\n",
    "            new_state = np.reshape(new_state, [1, num_observation_space])\n",
    "            current_state = new_state\n",
    "            reward_for_episode += reward\n",
    "            if done:\n",
    "                break\n",
    "        rewards_list.append(reward_for_episode)\n",
    "        print(test_episode, \"\\t: Episode || Reward: \", reward_for_episode)\n",
    "\n",
    "    return rewards_list\n",
    "\n",
    "\n",
    "def plot_df(df, chart_name, title, x_axis_label, y_axis_label):\n",
    "    plt.rcParams.update({'font.size': 17})\n",
    "    df['rolling_mean'] = df[df.columns[0]].rolling(100).mean()\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.close()\n",
    "    plt.figure()\n",
    "    # plot = df.plot(linewidth=1.5, figsize=(15, 8), title=title)\n",
    "    plot = df.plot(linewidth=1.5, figsize=(15, 8))\n",
    "    plot.set_xlabel(x_axis_label)\n",
    "    plot.set_ylabel(y_axis_label)\n",
    "    # plt.ylim((-400, 300))\n",
    "    fig = plot.get_figure()\n",
    "    plt.legend().set_visible(False)\n",
    "    fig.savefig(chart_name)\n",
    "\n",
    "\n",
    "def plot_df2(df, chart_name, title, x_axis_label, y_axis_label):\n",
    "    df['mean'] = df[df.columns[0]].mean()\n",
    "    plt.rcParams.update({'font.size': 17})\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.close()\n",
    "    plt.figure()\n",
    "    # plot = df.plot(linewidth=1.5, figsize=(15, 8), title=title)\n",
    "    plot = df.plot(linewidth=1.5, figsize=(15, 8))\n",
    "    plot.set_xlabel(x_axis_label)\n",
    "    plot.set_ylabel(y_axis_label)\n",
    "    plt.ylim((0, 300))\n",
    "    plt.xlim((0, 100))\n",
    "    plt.legend().set_visible(False)\n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig(chart_name)\n",
    "\n",
    "\n",
    "def plot_experiments(df, chart_name, title, x_axis_label, y_axis_label, y_limit):\n",
    "    plt.rcParams.update({'font.size': 17})\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.close()\n",
    "    plt.figure()\n",
    "    plot = df.plot(linewidth=1, figsize=(15, 8), title=title)\n",
    "    plot.set_xlabel(x_axis_label)\n",
    "    plot.set_ylabel(y_axis_label)\n",
    "    plt.ylim(y_limit)\n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig(chart_name)\n",
    "\n",
    "\n",
    "def run_experiment_for_gamma():\n",
    "    print('Running Experiment for gamma...')\n",
    "    env = gym.make('LunarLander-v2')\n",
    "\n",
    "    # set seeds\n",
    "    env.seed(21)\n",
    "    np.random.seed(21)\n",
    "\n",
    "    # setting up params\n",
    "    lr = 0.001\n",
    "    epsilon = 1.0\n",
    "    epsilon_decay = 0.995\n",
    "    gamma_list = [0.99, 0.9, 0.8, 0.7]\n",
    "    training_episodes = 1000\n",
    "\n",
    "    rewards_list_for_gammas = []\n",
    "    for gamma_value in gamma_list:\n",
    "        # save_dir = \"hp_gamma_\"+ str(gamma_value) + \"_\"\n",
    "        model = DQN(env, lr, gamma_value, epsilon, epsilon_decay)\n",
    "        print(\"Training model for Gamma: {}\".format(gamma_value))\n",
    "        model.train(training_episodes, False)\n",
    "        rewards_list_for_gammas.append(model.rewards_list)\n",
    "\n",
    "    pickle.dump(rewards_list_for_gammas, open(\"rewards_list_for_gammas.p\", \"wb\"))\n",
    "    rewards_list_for_gammas = pickle.load(open(\"rewards_list_for_gammas.p\", \"rb\"))\n",
    "\n",
    "    gamma_rewards_pd = pd.DataFrame(index=pd.Series(range(1, training_episodes + 1)))\n",
    "    for i in range(len(gamma_list)):\n",
    "        col_name = \"gamma=\" + str(gamma_list[i])\n",
    "        gamma_rewards_pd[col_name] = rewards_list_for_gammas[i]\n",
    "    plot_experiments(gamma_rewards_pd, \"Figure 4: Rewards per episode for different gamma values\",\n",
    "                     \"Figure 4: Rewards per episode for different gamma values\", \"Episodes\", \"Reward\", (-600, 300))\n",
    "\n",
    "\n",
    "def run_experiment_for_lr():\n",
    "    print('Running Experiment for learning rate...')\n",
    "    env = gym.make('LunarLander-v2')\n",
    "\n",
    "    # set seeds\n",
    "    env.seed(21)\n",
    "    np.random.seed(21)\n",
    "\n",
    "    # setting up params\n",
    "    lr_values = [0.0001, 0.001, 0.01, 0.1]\n",
    "    epsilon = 1.0\n",
    "    epsilon_decay = 0.995\n",
    "    gamma = 0.99\n",
    "    training_episodes = 1000\n",
    "    rewards_list_for_lrs = []\n",
    "    for lr_value in lr_values:\n",
    "        model = DQN(env, lr_value, gamma, epsilon, epsilon_decay)\n",
    "        print(\"Training model for LR: {}\".format(lr_value))\n",
    "        model.train(training_episodes, False)\n",
    "        rewards_list_for_lrs.append(model.rewards_list)\n",
    "\n",
    "    pickle.dump(rewards_list_for_lrs, open(\"rewards_list_for_lrs.p\", \"wb\"))\n",
    "    rewards_list_for_lrs = pickle.load(open(\"rewards_list_for_lrs.p\", \"rb\"))\n",
    "\n",
    "    lr_rewards_pd = pd.DataFrame(index=pd.Series(range(1, training_episodes + 1)))\n",
    "    for i in range(len(lr_values)):\n",
    "        col_name = \"lr=\"+ str(lr_values[i])\n",
    "        lr_rewards_pd[col_name] = rewards_list_for_lrs[i]\n",
    "    plot_experiments(lr_rewards_pd, \"Figure 3: Rewards per episode for different learning rates\", \"Figure 3: Rewards per episode for different learning rates\", \"Episodes\", \"Reward\", (-2000, 300))\n",
    "\n",
    "\n",
    "def run_experiment_for_ed():\n",
    "    print('Running Experiment for epsilon decay...')\n",
    "    env = gym.make('LunarLander-v2')\n",
    "\n",
    "    # set seeds\n",
    "    env.seed(21)\n",
    "    np.random.seed(21)\n",
    "\n",
    "    # setting up params\n",
    "    lr = 0.001\n",
    "    epsilon = 1.0\n",
    "    ed_values = [0.999, 0.995, 0.990, 0.9]\n",
    "    gamma = 0.99\n",
    "    training_episodes = 1000\n",
    "\n",
    "    rewards_list_for_ed = []\n",
    "    for ed in ed_values:\n",
    "        save_dir = \"hp_ed_\"+ str(ed) + \"_\"\n",
    "        model = DQN(env, lr, gamma, epsilon, ed)\n",
    "        print(\"Training model for ED: {}\".format(ed))\n",
    "        model.train(training_episodes, False)\n",
    "        rewards_list_for_ed.append(model.rewards_list)\n",
    "\n",
    "    pickle.dump(rewards_list_for_ed, open(\"rewards_list_for_ed.p\", \"wb\"))\n",
    "    rewards_list_for_ed = pickle.load(open(\"rewards_list_for_ed.p\", \"rb\"))\n",
    "\n",
    "    ed_rewards_pd = pd.DataFrame(index=pd.Series(range(1, training_episodes+1)))\n",
    "    for i in range(len(ed_values)):\n",
    "        col_name = \"epsilon_decay = \"+ str(ed_values[i])\n",
    "        ed_rewards_pd[col_name] = rewards_list_for_ed[i]\n",
    "    plot_experiments(ed_rewards_pd, \"Figure 5: Rewards per episode for different epsilon(ε) decay\", \"Figure 5: Rewards per episode for different epsilon(ε) decay values\", \"Episodes\", \"Reward\", (-600, 300))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    env = LunarLander()\n",
    "    \n",
    "    # set seeds\n",
    "    env.seed(21)\n",
    "    np.random.seed(21)\n",
    "\n",
    "    # setting up params\n",
    "    lr = 0.001\n",
    "    epsilon = 1.0\n",
    "    epsilon_decay = 0.995\n",
    "    gamma = 0.99\n",
    "    training_episodes = 2000\n",
    "    print('St')\n",
    "    model = DQN(env, lr, gamma, epsilon, epsilon_decay)\n",
    "    model.train(training_episodes, True)\n",
    "\n",
    "    # Save Everything\n",
    "    save_dir = \"saved_models\"\n",
    "    # Save trained model\n",
    "    model.save(save_dir + \"trained_model.h5\")\n",
    "\n",
    "    # Save Rewards list\n",
    "    pickle.dump(model.rewards_list, open(save_dir + \"train_rewards_list.p\", \"wb\"))\n",
    "    rewards_list = pickle.load(open(save_dir + \"train_rewards_list.p\", \"rb\"))\n",
    "\n",
    "    # plot reward in graph\n",
    "    reward_df = pd.DataFrame(rewards_list)\n",
    "    plot_df(reward_df, \"Figure 1: Reward for each training episode\", \"Reward for each training episode\", \"Episode\",\"Reward\")\n",
    "\n",
    "    # Test the model\n",
    "    trained_model = load_model(save_dir + \"trained_model.h5\")\n",
    "    test_rewards = test_already_trained_model(trained_model)\n",
    "    pickle.dump(test_rewards, open(save_dir + \"test_rewards.p\", \"wb\"))\n",
    "    test_rewards = pickle.load(open(save_dir + \"test_rewards.p\", \"rb\"))\n",
    "\n",
    "    plot_df2(pd.DataFrame(test_rewards), \"Figure 2: Reward for each testing episode\",\"Reward for each testing episode\", \"Episode\", \"Reward\")\n",
    "    print(\"Training and Testing Completed...!\")\n",
    "\n",
    "    # Run experiments for hyper-parameter\n",
    "    run_experiment_for_lr()\n",
    "    run_experiment_for_ed()\n",
    "    run_experiment_for_gamma()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
