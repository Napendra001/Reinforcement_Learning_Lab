{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "St\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               4608      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 136,964\n",
      "Trainable params: 136,964\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ribtas007/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "/home/ribtas007/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/ribtas007/.local/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t: Episode || Reward:  -225.4157144241999 \t|| Average Reward:  -225.4157144241999 \t epsilon:  0.995\n",
      "1 \t: Episode || Reward:  -100.29849027734899 \t|| Average Reward:  -162.85710235077445 \t epsilon:  0.990025\n",
      "2 \t: Episode || Reward:  -183.76696784463016 \t|| Average Reward:  -169.82705751539302 \t epsilon:  0.985074875\n",
      "3 \t: Episode || Reward:  -210.04639151915842 \t|| Average Reward:  -179.88189101633438 \t epsilon:  0.9801495006250001\n",
      "4 \t: Episode || Reward:  -180.91921531171863 \t|| Average Reward:  -180.08935587541123 \t epsilon:  0.9752487531218751\n",
      "5 \t: Episode || Reward:  -180.71779893546034 \t|| Average Reward:  -180.19409638541939 \t epsilon:  0.9703725093562657\n",
      "6 \t: Episode || Reward:  -141.85726231461967 \t|| Average Reward:  -174.71740580387657 \t epsilon:  0.9655206468094844\n",
      "7 \t: Episode || Reward:  -97.39907915168925 \t|| Average Reward:  -165.05261497235315 \t epsilon:  0.960693043575437\n",
      "8 \t: Episode || Reward:  -201.0775838476469 \t|| Average Reward:  -169.05538929183024 \t epsilon:  0.9558895783575597\n",
      "9 \t: Episode || Reward:  -340.6953980694231 \t|| Average Reward:  -186.21939016958953 \t epsilon:  0.9511101304657719\n",
      "10 \t: Episode || Reward:  -79.09747814394018 \t|| Average Reward:  -176.48103453089414 \t epsilon:  0.946354579813443\n",
      "11 \t: Episode || Reward:  -112.59428184197648 \t|| Average Reward:  -171.15713847348434 \t epsilon:  0.9416228069143757\n",
      "12 \t: Episode || Reward:  -106.64787879191428 \t|| Average Reward:  -166.19488772874817 \t epsilon:  0.9369146928798039\n",
      "13 \t: Episode || Reward:  -47.44293087632403 \t|| Average Reward:  -157.71260509643216 \t epsilon:  0.9322301194154049\n",
      "14 \t: Episode || Reward:  -22.053443042046638 \t|| Average Reward:  -148.66866095947313 \t epsilon:  0.9275689688183278\n",
      "15 \t: Episode || Reward:  -104.14222974547127 \t|| Average Reward:  -145.88575900859803 \t epsilon:  0.9229311239742362\n",
      "16 \t: Episode || Reward:  -133.1622420915183 \t|| Average Reward:  -145.1373168370051 \t epsilon:  0.918316468354365\n",
      "17 \t: Episode || Reward:  -162.26556896444254 \t|| Average Reward:  -146.0888863996405 \t epsilon:  0.9137248860125932\n",
      "18 \t: Episode || Reward:  1.0703616571538674 \t|| Average Reward:  -138.34366281770394 \t epsilon:  0.9091562615825302\n",
      "19 \t: Episode || Reward:  -198.16418067417248 \t|| Average Reward:  -141.33468871052736 \t epsilon:  0.9046104802746175\n",
      "20 \t: Episode || Reward:  -253.04761922152704 \t|| Average Reward:  -146.65435206819402 \t epsilon:  0.9000874278732445\n",
      "21 \t: Episode || Reward:  -261.8222216973942 \t|| Average Reward:  -151.88925523315766 \t epsilon:  0.8955869907338783\n",
      "22 \t: Episode || Reward:  -142.12261231490555 \t|| Average Reward:  -151.464618584538 \t epsilon:  0.8911090557802088\n",
      "23 \t: Episode || Reward:  -148.80218905909578 \t|| Average Reward:  -151.3536840209779 \t epsilon:  0.8866535105013078\n",
      "24 \t: Episode || Reward:  -363.82179674836607 \t|| Average Reward:  -159.85240853007343 \t epsilon:  0.8822202429488013\n",
      "25 \t: Episode || Reward:  -57.758771677124926 \t|| Average Reward:  -155.92573018957543 \t epsilon:  0.8778091417340573\n",
      "26 \t: Episode || Reward:  -91.92969134016688 \t|| Average Reward:  -153.5555065284862 \t epsilon:  0.8734200960253871\n",
      "27 \t: Episode || Reward:  -372.14792014252066 \t|| Average Reward:  -161.36237844327317 \t epsilon:  0.8690529955452602\n",
      "28 \t: Episode || Reward:  -234.44417166068013 \t|| Average Reward:  -163.88244027835614 \t epsilon:  0.8647077305675338\n",
      "29 \t: Episode || Reward:  -104.19835257532597 \t|| Average Reward:  -161.89297068825513 \t epsilon:  0.8603841919146962\n",
      "30 \t: Episode || Reward:  -155.2306189989373 \t|| Average Reward:  -161.678056117632 \t epsilon:  0.8560822709551227\n",
      "31 \t: Episode || Reward:  -352.4211356157132 \t|| Average Reward:  -167.63877735194706 \t epsilon:  0.851801859600347\n",
      "32 \t: Episode || Reward:  -518.5973855498544 \t|| Average Reward:  -178.27388669127757 \t epsilon:  0.8475428503023453\n",
      "33 \t: Episode || Reward:  -222.97744603776846 \t|| Average Reward:  -179.588697260292 \t epsilon:  0.8433051360508336\n",
      "34 \t: Episode || Reward:  -30.433831897637134 \t|| Average Reward:  -175.32712967850188 \t epsilon:  0.8390886103705794\n",
      "35 \t: Episode || Reward:  -81.06501874403757 \t|| Average Reward:  -172.7087377081001 \t epsilon:  0.8348931673187264\n",
      "36 \t: Episode || Reward:  -415.099640107994 \t|| Average Reward:  -179.25984317836748 \t epsilon:  0.8307187014821328\n",
      "37 \t: Episode || Reward:  -140.1385170627041 \t|| Average Reward:  -178.23033459637637 \t epsilon:  0.8265651079747222\n",
      "38 \t: Episode || Reward:  -88.09389598551215 \t|| Average Reward:  -175.91914386276446 \t epsilon:  0.8224322824348486\n",
      "39 \t: Episode || Reward:  -110.69085323643698 \t|| Average Reward:  -174.28843659710628 \t epsilon:  0.8183201210226743\n",
      "40 \t: Episode || Reward:  -114.83254667891822 \t|| Average Reward:  -172.8382929405651 \t epsilon:  0.8142285204175609\n",
      "41 \t: Episode || Reward:  -172.10975769021735 \t|| Average Reward:  -172.8209468631759 \t epsilon:  0.810157377815473\n",
      "42 \t: Episode || Reward:  -114.88926214530869 \t|| Average Reward:  -171.473698381365 \t epsilon:  0.8061065909263957\n",
      "43 \t: Episode || Reward:  -137.9723322805893 \t|| Average Reward:  -170.7123036972565 \t epsilon:  0.8020760579717637\n",
      "44 \t: Episode || Reward:  -184.73922006127754 \t|| Average Reward:  -171.0240129497903 \t epsilon:  0.798065677681905\n",
      "45 \t: Episode || Reward:  -208.38144299065158 \t|| Average Reward:  -171.83613099415683 \t epsilon:  0.7940753492934954\n",
      "46 \t: Episode || Reward:  -130.0886015123609 \t|| Average Reward:  -170.94788568603352 \t epsilon:  0.7901049725470279\n",
      "47 \t: Episode || Reward:  -90.60269298269087 \t|| Average Reward:  -169.2740275047138 \t epsilon:  0.7861544476842928\n",
      "48 \t: Episode || Reward:  -144.52982142179306 \t|| Average Reward:  -168.76904370710318 \t epsilon:  0.7822236754458713\n",
      "49 \t: Episode || Reward:  -101.79912924208108 \t|| Average Reward:  -167.42964541780276 \t epsilon:  0.778312557068642\n",
      "50 \t: Episode || Reward:  -242.6407837878926 \t|| Average Reward:  -168.90437362113784 \t epsilon:  0.7744209942832988\n",
      "51 \t: Episode || Reward:  -26.01652996968373 \t|| Average Reward:  -166.15653047399448 \t epsilon:  0.7705488893118823\n",
      "52 \t: Episode || Reward:  -99.09716012034568 \t|| Average Reward:  -164.8912593352464 \t epsilon:  0.7666961448653229\n",
      "53 \t: Episode || Reward:  -199.61950045910692 \t|| Average Reward:  -165.53437491161418 \t epsilon:  0.7628626641409962\n",
      "54 \t: Episode || Reward:  -138.61743104661366 \t|| Average Reward:  -165.04497593225054 \t epsilon:  0.7590483508202912\n",
      "55 \t: Episode || Reward:  -34.571965077837916 \t|| Average Reward:  -162.71510073842177 \t epsilon:  0.7552531090661897\n",
      "56 \t: Episode || Reward:  -18.87681789430198 \t|| Average Reward:  -160.1916220920337 \t epsilon:  0.7514768435208588\n",
      "57 \t: Episode || Reward:  -148.72590733252684 \t|| Average Reward:  -159.99393735480083 \t epsilon:  0.7477194593032545\n",
      "58 \t: Episode || Reward:  -113.46079495825526 \t|| Average Reward:  -159.20524002604583 \t epsilon:  0.7439808620067382\n",
      "59 \t: Episode || Reward:  -300.0971402634634 \t|| Average Reward:  -161.55343836333614 \t epsilon:  0.7402609576967045\n",
      "60 \t: Episode || Reward:  -146.75005134430467 \t|| Average Reward:  -161.3107598876143 \t epsilon:  0.736559652908221\n",
      "61 \t: Episode || Reward:  -69.71414528639315 \t|| Average Reward:  -159.8333951359817 \t epsilon:  0.7328768546436799\n",
      "62 \t: Episode || Reward:  -115.06990947718812 \t|| Average Reward:  -159.12286361758814 \t epsilon:  0.7292124703704616\n",
      "63 \t: Episode || Reward:  -144.39395064788783 \t|| Average Reward:  -158.89272435243657 \t epsilon:  0.7255664080186093\n",
      "64 \t: Episode || Reward:  -80.54525009343156 \t|| Average Reward:  -157.68737859460572 \t epsilon:  0.7219385759785162\n",
      "65 \t: Episode || Reward:  -67.86707506999207 \t|| Average Reward:  -156.32646490483884 \t epsilon:  0.7183288830986236\n",
      "66 \t: Episode || Reward:  -65.8426769264644 \t|| Average Reward:  -154.97596060665416 \t epsilon:  0.7147372386831305\n",
      "67 \t: Episode || Reward:  -48.321788114847024 \t|| Average Reward:  -153.40751689353934 \t epsilon:  0.7111635524897149\n",
      "68 \t: Episode || Reward:  -111.60008415144327 \t|| Average Reward:  -152.8016120711901 \t epsilon:  0.7076077347272662\n",
      "69 \t: Episode || Reward:  -53.56219161491276 \t|| Average Reward:  -151.38390606467186 \t epsilon:  0.7040696960536299\n",
      "70 \t: Episode || Reward:  -91.62668837433782 \t|| Average Reward:  -150.54225511128686 \t epsilon:  0.7005493475733617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 \t: Episode || Reward:  -64.24626898790628 \t|| Average Reward:  -149.34369974846217 \t epsilon:  0.697046600835495\n",
      "72 \t: Episode || Reward:  -71.8187563531475 \t|| Average Reward:  -148.28171422249895 \t epsilon:  0.6935613678313175\n",
      "73 \t: Episode || Reward:  -81.53895551424442 \t|| Average Reward:  -147.3797850507658 \t epsilon:  0.6900935609921609\n",
      "74 \t: Episode || Reward:  -64.87017696979856 \t|| Average Reward:  -146.27965694301955 \t epsilon:  0.6866430931872001\n",
      "75 \t: Episode || Reward:  -21.211195313397695 \t|| Average Reward:  -144.6340192899982 \t epsilon:  0.6832098777212641\n",
      "76 \t: Episode || Reward:  -108.44398262770974 \t|| Average Reward:  -144.1640188138646 \t epsilon:  0.6797938283326578\n",
      "77 \t: Episode || Reward:  -119.17723771938515 \t|| Average Reward:  -143.84367546649946 \t epsilon:  0.6763948591909945\n",
      "78 \t: Episode || Reward:  -93.09801197914037 \t|| Average Reward:  -143.20132529577342 \t epsilon:  0.6730128848950395\n",
      "79 \t: Episode || Reward:  -83.01327313053997 \t|| Average Reward:  -142.44897464370803 \t epsilon:  0.6696478204705644\n",
      "80 \t: Episode || Reward:  -75.13926240584344 \t|| Average Reward:  -141.61799054200597 \t epsilon:  0.6662995813682115\n",
      "81 \t: Episode || Reward:  -43.81309084908972 \t|| Average Reward:  -140.42524786282405 \t epsilon:  0.6629680834613705\n",
      "82 \t: Episode || Reward:  -39.441674517344346 \t|| Average Reward:  -139.20857830444479 \t epsilon:  0.6596532430440636\n",
      "83 \t: Episode || Reward:  -48.87980065551626 \t|| Average Reward:  -138.1332357133861 \t epsilon:  0.6563549768288433\n",
      "84 \t: Episode || Reward:  -39.43331440950274 \t|| Average Reward:  -136.97206016863456 \t epsilon:  0.653073201944699\n",
      "85 \t: Episode || Reward:  -66.80005533562263 \t|| Average Reward:  -136.15610662406468 \t epsilon:  0.6498078359349755\n",
      "86 \t: Episode || Reward:  -123.51896910591957 \t|| Average Reward:  -136.01085216983313 \t epsilon:  0.6465587967553006\n",
      "87 \t: Episode || Reward:  -225.84656711202592 \t|| Average Reward:  -137.03171256690345 \t epsilon:  0.6433260027715241\n",
      "88 \t: Episode || Reward:  -70.63527983914607 \t|| Average Reward:  -136.28568523288368 \t epsilon:  0.6401093727576664\n",
      "89 \t: Episode || Reward:  -73.77683355081203 \t|| Average Reward:  -135.59114243641622 \t epsilon:  0.6369088258938781\n",
      "90 \t: Episode || Reward:  -76.79946634914178 \t|| Average Reward:  -134.9450800618308 \t epsilon:  0.6337242817644086\n",
      "91 \t: Episode || Reward:  -85.44883743216512 \t|| Average Reward:  -134.4070774245518 \t epsilon:  0.6305556603555866\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.activations import relu, linear\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, env, lr, gamma, epsilon, epsilon_decay):\n",
    "\n",
    "        self.env = env\n",
    "        self.action_space = env.action_space\n",
    "        self.observation_space = env.observation_space\n",
    "        self.counter = 0\n",
    "\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.rewards_list = []\n",
    "\n",
    "        self.replay_memory_buffer = deque(maxlen=500000)\n",
    "        self.batch_size = 64\n",
    "        self.epsilon_min = 0.01\n",
    "        self.num_action_space = self.action_space.n\n",
    "        self.num_observation_space = env.observation_space.shape[0]\n",
    "        self.model = self.initialize_model()\n",
    "\n",
    "    def initialize_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(512, input_dim=self.num_observation_space, activation=relu))\n",
    "        model.add(Dense(256, activation=relu))\n",
    "        model.add(Dense(self.num_action_space, activation=linear))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss=mean_squared_error,optimizer=Adam(lr=self.lr))\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.num_action_space)\n",
    "\n",
    "        predicted_actions = self.model.predict(state)\n",
    "        return np.argmax(predicted_actions[0])\n",
    "\n",
    "    def add_to_replay_memory(self, state, action, reward, next_state, done):\n",
    "        self.replay_memory_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def learn_and_update_weights_by_reply(self):\n",
    "\n",
    "        # replay_memory_buffer size check\n",
    "        if len(self.replay_memory_buffer) < self.batch_size or self.counter != 0:\n",
    "            return\n",
    "\n",
    "        # Early Stopping\n",
    "        if np.mean(self.rewards_list[-10:]) > 180:\n",
    "            return\n",
    "\n",
    "        random_sample = self.get_random_sample_from_replay_mem()\n",
    "        states, actions, rewards, next_states, done_list = self.get_attribues_from_sample(random_sample)\n",
    "        targets = rewards + self.gamma * (np.amax(self.model.predict_on_batch(next_states), axis=1)) * (1 - done_list)\n",
    "        target_vec = self.model.predict_on_batch(states)\n",
    "        indexes = np.array([i for i in range(self.batch_size)])\n",
    "        target_vec[[indexes], [actions]] = targets\n",
    "\n",
    "        self.model.fit(states, target_vec, epochs=1, verbose=0)\n",
    "\n",
    "    def get_attribues_from_sample(self, random_sample):\n",
    "        states = np.array([i[0] for i in random_sample])\n",
    "        actions = np.array([i[1] for i in random_sample])\n",
    "        rewards = np.array([i[2] for i in random_sample])\n",
    "        next_states = np.array([i[3] for i in random_sample])\n",
    "        done_list = np.array([i[4] for i in random_sample])\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "        return np.squeeze(states), actions, rewards, next_states, done_list\n",
    "\n",
    "    def get_random_sample_from_replay_mem(self):\n",
    "        random_sample = random.sample(self.replay_memory_buffer, self.batch_size)\n",
    "        return random_sample\n",
    "\n",
    "    def train(self, num_episodes=2000, can_stop=True):\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            reward_for_episode = 0\n",
    "            num_steps = 1000\n",
    "            state = np.reshape(state, [1, self.num_observation_space])\n",
    "            for step in range(num_steps):\n",
    "                env.render()\n",
    "                received_action = self.get_action(state)\n",
    "                # print(\"received_action:\", received_action)\n",
    "                next_state, reward, done, info = env.step(received_action)\n",
    "                next_state = np.reshape(next_state, [1, self.num_observation_space])\n",
    "                # Store the experience in replay memory\n",
    "                self.add_to_replay_memory(state, received_action, reward, next_state, done)\n",
    "                # add up rewards\n",
    "                reward_for_episode += reward\n",
    "                state = next_state\n",
    "                self.update_counter()\n",
    "                self.learn_and_update_weights_by_reply()\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "            self.rewards_list.append(reward_for_episode)\n",
    "\n",
    "            # Decay the epsilon after each experience completion\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "\n",
    "            # Check for breaking condition\n",
    "            last_rewards_mean = np.mean(self.rewards_list[-100:])\n",
    "            if last_rewards_mean > 200 and can_stop:\n",
    "                print(\"DQN Training Complete...\")\n",
    "                break\n",
    "            print(episode, \"\\t: Episode || Reward: \",reward_for_episode, \"\\t|| Average Reward: \",last_rewards_mean, \"\\t epsilon: \", self.epsilon )\n",
    "\n",
    "    def update_counter(self):\n",
    "        self.counter += 1\n",
    "        step_size = 5\n",
    "        self.counter = self.counter % step_size\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "\n",
    "\n",
    "def test_already_trained_model(trained_model):\n",
    "    rewards_list = []\n",
    "    num_test_episode = 100\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "    print(\"Starting Testing of the trained model...\")\n",
    "\n",
    "    step_count = 1000\n",
    "\n",
    "    for test_episode in range(num_test_episode):\n",
    "        current_state = env.reset()\n",
    "        num_observation_space = env.observation_space.shape[0]\n",
    "        current_state = np.reshape(current_state, [1, num_observation_space])\n",
    "        reward_for_episode = 0\n",
    "        for step in range(step_count):\n",
    "            env.render()\n",
    "            selected_action = np.argmax(trained_model.predict(current_state)[0])\n",
    "            new_state, reward, done, info = env.step(selected_action)\n",
    "            new_state = np.reshape(new_state, [1, num_observation_space])\n",
    "            current_state = new_state\n",
    "            reward_for_episode += reward\n",
    "            if done:\n",
    "                break\n",
    "        rewards_list.append(reward_for_episode)\n",
    "        print(test_episode, \"\\t: Episode || Reward: \", reward_for_episode)\n",
    "\n",
    "    return rewards_list\n",
    "\n",
    "\n",
    "def plot_df(df, chart_name, title, x_axis_label, y_axis_label):\n",
    "    plt.rcParams.update({'font.size': 17})\n",
    "    df['rolling_mean'] = df[df.columns[0]].rolling(100).mean()\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.close()\n",
    "    plt.figure()\n",
    "    # plot = df.plot(linewidth=1.5, figsize=(15, 8), title=title)\n",
    "    plot = df.plot(linewidth=1.5, figsize=(15, 8))\n",
    "    plot.set_xlabel(x_axis_label)\n",
    "    plot.set_ylabel(y_axis_label)\n",
    "    # plt.ylim((-400, 300))\n",
    "    fig = plot.get_figure()\n",
    "    plt.legend().set_visible(False)\n",
    "    fig.savefig(chart_name)\n",
    "\n",
    "\n",
    "def plot_df2(df, chart_name, title, x_axis_label, y_axis_label):\n",
    "    df['mean'] = df[df.columns[0]].mean()\n",
    "    plt.rcParams.update({'font.size': 17})\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.close()\n",
    "    plt.figure()\n",
    "    # plot = df.plot(linewidth=1.5, figsize=(15, 8), title=title)\n",
    "    plot = df.plot(linewidth=1.5, figsize=(15, 8))\n",
    "    plot.set_xlabel(x_axis_label)\n",
    "    plot.set_ylabel(y_axis_label)\n",
    "    plt.ylim((0, 300))\n",
    "    plt.xlim((0, 100))\n",
    "    plt.legend().set_visible(False)\n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig(chart_name)\n",
    "\n",
    "\n",
    "def plot_experiments(df, chart_name, title, x_axis_label, y_axis_label, y_limit):\n",
    "    plt.rcParams.update({'font.size': 17})\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.close()\n",
    "    plt.figure()\n",
    "    plot = df.plot(linewidth=1, figsize=(15, 8), title=title)\n",
    "    plot.set_xlabel(x_axis_label)\n",
    "    plot.set_ylabel(y_axis_label)\n",
    "    plt.ylim(y_limit)\n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig(chart_name)\n",
    "\n",
    "\n",
    "def run_experiment_for_gamma():\n",
    "    print('Running Experiment for gamma...')\n",
    "    env = gym.make('LunarLander-v2')\n",
    "\n",
    "    # set seeds\n",
    "    env.seed(21)\n",
    "    np.random.seed(21)\n",
    "\n",
    "    # setting up params\n",
    "    lr = 0.001\n",
    "    epsilon = 1.0\n",
    "    epsilon_decay = 0.995\n",
    "    gamma_list = [0.99, 0.9, 0.8, 0.7]\n",
    "    training_episodes = 1000\n",
    "\n",
    "    rewards_list_for_gammas = []\n",
    "    for gamma_value in gamma_list:\n",
    "        # save_dir = \"hp_gamma_\"+ str(gamma_value) + \"_\"\n",
    "        model = DQN(env, lr, gamma_value, epsilon, epsilon_decay)\n",
    "        print(\"Training model for Gamma: {}\".format(gamma_value))\n",
    "        model.train(training_episodes, False)\n",
    "        rewards_list_for_gammas.append(model.rewards_list)\n",
    "\n",
    "    pickle.dump(rewards_list_for_gammas, open(\"rewards_list_for_gammas.p\", \"wb\"))\n",
    "    rewards_list_for_gammas = pickle.load(open(\"rewards_list_for_gammas.p\", \"rb\"))\n",
    "\n",
    "    gamma_rewards_pd = pd.DataFrame(index=pd.Series(range(1, training_episodes + 1)))\n",
    "    for i in range(len(gamma_list)):\n",
    "        col_name = \"gamma=\" + str(gamma_list[i])\n",
    "        gamma_rewards_pd[col_name] = rewards_list_for_gammas[i]\n",
    "    plot_experiments(gamma_rewards_pd, \"Figure 4: Rewards per episode for different gamma values\",\n",
    "                     \"Figure 4: Rewards per episode for different gamma values\", \"Episodes\", \"Reward\", (-600, 300))\n",
    "\n",
    "\n",
    "def run_experiment_for_lr():\n",
    "    print('Running Experiment for learning rate...')\n",
    "    env = gym.make('LunarLander-v2')\n",
    "\n",
    "    # set seeds\n",
    "    env.seed(21)\n",
    "    np.random.seed(21)\n",
    "\n",
    "    # setting up params\n",
    "    lr_values = [0.0001, 0.001, 0.01, 0.1]\n",
    "    epsilon = 1.0\n",
    "    epsilon_decay = 0.995\n",
    "    gamma = 0.99\n",
    "    training_episodes = 1000\n",
    "    rewards_list_for_lrs = []\n",
    "    for lr_value in lr_values:\n",
    "        model = DQN(env, lr_value, gamma, epsilon, epsilon_decay)\n",
    "        print(\"Training model for LR: {}\".format(lr_value))\n",
    "        model.train(training_episodes, False)\n",
    "        rewards_list_for_lrs.append(model.rewards_list)\n",
    "\n",
    "    pickle.dump(rewards_list_for_lrs, open(\"rewards_list_for_lrs.p\", \"wb\"))\n",
    "    rewards_list_for_lrs = pickle.load(open(\"rewards_list_for_lrs.p\", \"rb\"))\n",
    "\n",
    "    lr_rewards_pd = pd.DataFrame(index=pd.Series(range(1, training_episodes + 1)))\n",
    "    for i in range(len(lr_values)):\n",
    "        col_name = \"lr=\"+ str(lr_values[i])\n",
    "        lr_rewards_pd[col_name] = rewards_list_for_lrs[i]\n",
    "    plot_experiments(lr_rewards_pd, \"Figure 3: Rewards per episode for different learning rates\", \"Figure 3: Rewards per episode for different learning rates\", \"Episodes\", \"Reward\", (-2000, 300))\n",
    "\n",
    "\n",
    "def run_experiment_for_ed():\n",
    "    print('Running Experiment for epsilon decay...')\n",
    "    env = gym.make('LunarLander-v2')\n",
    "\n",
    "    # set seeds\n",
    "    env.seed(21)\n",
    "    np.random.seed(21)\n",
    "\n",
    "    # setting up params\n",
    "    lr = 0.001\n",
    "    epsilon = 1.0\n",
    "    ed_values = [0.999, 0.995, 0.990, 0.9]\n",
    "    gamma = 0.99\n",
    "    training_episodes = 1000\n",
    "\n",
    "    rewards_list_for_ed = []\n",
    "    for ed in ed_values:\n",
    "        save_dir = \"hp_ed_\"+ str(ed) + \"_\"\n",
    "        model = DQN(env, lr, gamma, epsilon, ed)\n",
    "        print(\"Training model for ED: {}\".format(ed))\n",
    "        model.train(training_episodes, False)\n",
    "        rewards_list_for_ed.append(model.rewards_list)\n",
    "\n",
    "    pickle.dump(rewards_list_for_ed, open(\"rewards_list_for_ed.p\", \"wb\"))\n",
    "    rewards_list_for_ed = pickle.load(open(\"rewards_list_for_ed.p\", \"rb\"))\n",
    "\n",
    "    ed_rewards_pd = pd.DataFrame(index=pd.Series(range(1, training_episodes+1)))\n",
    "    for i in range(len(ed_values)):\n",
    "        col_name = \"epsilon_decay = \"+ str(ed_values[i])\n",
    "        ed_rewards_pd[col_name] = rewards_list_for_ed[i]\n",
    "    plot_experiments(ed_rewards_pd, \"Figure 5: Rewards per episode for different epsilon(ε) decay\", \"Figure 5: Rewards per episode for different epsilon(ε) decay values\", \"Episodes\", \"Reward\", (-600, 300))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('LunarLander-v2')\n",
    "\n",
    "    # set seeds\n",
    "    env.seed(21)\n",
    "    np.random.seed(21)\n",
    "\n",
    "    # setting up params\n",
    "    lr = 0.001\n",
    "    epsilon = 1.0\n",
    "    epsilon_decay = 0.995\n",
    "    gamma = 0.99\n",
    "    training_episodes = 2000\n",
    "    print('St')\n",
    "    model = DQN(env, lr, gamma, epsilon, epsilon_decay)\n",
    "    model.train(training_episodes, True)\n",
    "\n",
    "    # Save Everything\n",
    "    save_dir = \"saved_models\"\n",
    "    # Save trained model\n",
    "    model.save(save_dir + \"trained_model.h5\")\n",
    "\n",
    "    # Save Rewards list\n",
    "    pickle.dump(model.rewards_list, open(save_dir + \"train_rewards_list.p\", \"wb\"))\n",
    "    rewards_list = pickle.load(open(save_dir + \"train_rewards_list.p\", \"rb\"))\n",
    "\n",
    "    # plot reward in graph\n",
    "    reward_df = pd.DataFrame(rewards_list)\n",
    "    plot_df(reward_df, \"Figure 1: Reward for each training episode\", \"Reward for each training episode\", \"Episode\",\"Reward\")\n",
    "\n",
    "    # Test the model\n",
    "    trained_model = load_model(save_dir + \"trained_model.h5\")\n",
    "    test_rewards = test_already_trained_model(trained_model)\n",
    "    pickle.dump(test_rewards, open(save_dir + \"test_rewards.p\", \"wb\"))\n",
    "    test_rewards = pickle.load(open(save_dir + \"test_rewards.p\", \"rb\"))\n",
    "\n",
    "    plot_df2(pd.DataFrame(test_rewards), \"Figure 2: Reward for each testing episode\",\"Reward for each testing episode\", \"Episode\", \"Reward\")\n",
    "    print(\"Training and Testing Completed...!\")\n",
    "\n",
    "    # Run experiments for hyper-parameter\n",
    "    run_experiment_for_lr()\n",
    "    run_experiment_for_ed()\n",
    "    run_experiment_for_gamma()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
